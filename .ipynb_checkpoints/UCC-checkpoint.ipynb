{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = ['sentiment','id','date','query_string','user','text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"training.csv\", header=None, names=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query_string</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                          date query_string  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009     NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009     NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009     NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009     NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009     NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    183533\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(['id','date','query_string','user'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pre_clean_len'] = [len(t) for t in df.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_shape': (183533, 3),\n",
      " 'pre_clean_len': {'description': 'Length of the tweet before cleaning',\n",
      "                   'type': dtype('int64')},\n",
      " 'sentiment': {'description': 'sentiment class - 0:negative, 1:positive',\n",
      "               'type': dtype('int64')},\n",
      " 'text': {'description': 'tweet text', 'type': dtype('O')}}\n"
     ]
    }
   ],
   "source": [
    "data_dict = {\n",
    "    'sentiment':{\n",
    "        'type':df.sentiment.dtype,\n",
    "        'description':'sentiment class - 0:negative, 1:positive'\n",
    "    },\n",
    "    'text':{\n",
    "        'type':df.text.dtype,\n",
    "        'description':'tweet text'\n",
    "    },\n",
    "    'pre_clean_len':{\n",
    "        'type':df.pre_clean_len.dtype,\n",
    "        'description':'Length of the tweet before cleaning'\n",
    "    },\n",
    "    'dataset_shape':df.shape\n",
    "}\n",
    "\n",
    "pprint(data_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUMAAAEyCAYAAABgTbJjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f945201a358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plt.boxplot(df.pre_clean_len)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>pre_clean_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0</td>\n",
       "      <td>Awwh babs... you look so sad underneith that s...</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>0</td>\n",
       "      <td>Whinging. My client&amp;amp;boss don't understand ...</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>0</td>\n",
       "      <td>@TheLeagueSF Not Fun &amp;amp; Furious? The new ma...</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>0</td>\n",
       "      <td>#3 woke up and was having an accident - &amp;quot;...</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>0</td>\n",
       "      <td>My bathtub drain is fired: it haz 1 job 2 do, ...</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>0</td>\n",
       "      <td>pears &amp;amp; Brie, bottle of Cabernet, and &amp;quo...</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>0</td>\n",
       "      <td>Have an invite for &amp;quot;Healthy Dining&amp;quot; ...</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>0</td>\n",
       "      <td>Damnit I was really digging this season of Rea...</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>0</td>\n",
       "      <td>Why do I keep looking...I know that what I rea...</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>0</td>\n",
       "      <td>Used the term &amp;quot;Fail Whale&amp;quot; to a clie...</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                               text  \\\n",
       "201           0  Awwh babs... you look so sad underneith that s...   \n",
       "267           0  Whinging. My client&amp;boss don't understand ...   \n",
       "331           0  @TheLeagueSF Not Fun &amp; Furious? The new ma...   \n",
       "388           0  #3 woke up and was having an accident - &quot;...   \n",
       "452           0  My bathtub drain is fired: it haz 1 job 2 do, ...   \n",
       "480           0  pears &amp; Brie, bottle of Cabernet, and &quo...   \n",
       "735           0  Have an invite for &quot;Healthy Dining&quot; ...   \n",
       "945           0  Damnit I was really digging this season of Rea...   \n",
       "1052          0  Why do I keep looking...I know that what I rea...   \n",
       "1059          0  Used the term &quot;Fail Whale&quot; to a clie...   \n",
       "\n",
       "      pre_clean_len  \n",
       "201             142  \n",
       "267             145  \n",
       "331             145  \n",
       "388             144  \n",
       "452             146  \n",
       "480             150  \n",
       "735             141  \n",
       "945             141  \n",
       "1052            141  \n",
       "1059            148  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.pre_clean_len > 140].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm afraid I had bad code. \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "example1 = BeautifulSoup(df.text[279], 'lxml')\n",
    "print (example1.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://twitpic.com/2y2yi - I love you, Buck. '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.sub(r'@[A-Za-z0-9]+','',df.text[343])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@switchfoot  - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('https?://[A-Za-z0-9./]+','',df.text[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ColinDeMar Far too out of the way for rail   any other tips '"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(\"[^a-zA-Z]\", \" \", df.text[164])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['awww that s a bummer you shoulda got david carr of third day to do it d',\n",
       " 'is upset that he can t update his facebook by texting it and might cry as a result school today also blah',\n",
       " 'i dived many times for the ball managed to save the rest go out of bounds',\n",
       " 'my whole body feels itchy and like its on fire',\n",
       " 'no it s not behaving at all i m mad why am i here because i can t see you all over there',\n",
       " 'not the whole crew',\n",
       " 'need a hug',\n",
       " 'hey long time no see yes rains a bit only a bit lol i m fine thanks how s you',\n",
       " 'k nope they didn t have it',\n",
       " 'que me muera',\n",
       " 'spring break in plain city it s snowing',\n",
       " 'i just re pierced my ears',\n",
       " 'i couldn t bear to watch it and i thought the ua loss was embarrassing',\n",
       " 'it it counts idk why i did either you never talk to me anymore',\n",
       " 'i would ve been the first but i didn t have a gun not really though zac snyder s just a doucheclown',\n",
       " 'i wish i got to watch it with you i miss you and how was the premiere',\n",
       " 'hollis death scene will hurt me severely to watch on film wry is directors cut not out now',\n",
       " 'about to file taxes',\n",
       " 'ahh ive always wanted to see rent love the soundtrack',\n",
       " 'oh dear were you drinking out of the forgotten table drinks',\n",
       " 'i was out most of the day so didn t get much done',\n",
       " 'one of my friend called me and asked to meet with her at mid valley today but i ve no time sigh',\n",
       " 'barista i baked you a cake but i ated it',\n",
       " 'this week is not going as i had hoped',\n",
       " 'blagh class at tomorrow',\n",
       " 'i hate when i have to call and wake people up',\n",
       " 'just going to cry myself to sleep after watching marley and me',\n",
       " 'im sad now miss lilly',\n",
       " 'ooooh lol that leslie and ok i won t do it again so leslie won t get mad again',\n",
       " 'meh almost lover is the exception this track gets me depressed every time',\n",
       " 'some hacked my account on aim now i have to make a new one',\n",
       " 'i want to go to promote gear and groove but unfornately no ride there i may b going to the one in anaheim in may though',\n",
       " 'thought sleeping in was an option tomorrow but realizing that it now is not evaluations in the morning and work in the afternoon',\n",
       " 'awe i love you too am here i miss you',\n",
       " 'i cry my asian eyes to sleep at night',\n",
       " 'ok i m sick and spent an hour sitting in the shower cause i was too sick to stand and held back the puke like a champ bed now',\n",
       " 'ill tell ya the story later not a good day and ill be workin for like three more hours',\n",
       " 'sorry bed time came here gmt',\n",
       " 'i don t either its depressing i don t think i even want to know about the kids in suitcases',\n",
       " 'bed class work gym or then class another day that s gonna fly by i miss my girlfriend',\n",
       " 'really don t feel like getting up today but got to study to for tomorrows practical exam',\n",
       " 'he s the reason for the teardrops on my guitar the only one who has enough of me to break my heart',\n",
       " 'sad sad sad i don t know why but i hate this feeling i wanna sleep and i still can t',\n",
       " 'awww i soo wish i was there to see you finally comfortable im sad that i missed it',\n",
       " 'falling asleep just heard about that tracy girl s body being found how sad my heart breaks for that family',\n",
       " 'yay i m happy for you with your job but that also means less time for me and you',\n",
       " 'just checked my user timeline on my blackberry it looks like the twanking is still happening are ppl still having probs w bgs and uids',\n",
       " 'oh man was ironing s fave top to wear to a meeting burnt it',\n",
       " 'is strangely sad about lilo and samro breaking up',\n",
       " 'oh i m so sorry i didn t think about that before retweeting',\n",
       " 'broadband plan a massive broken promise via www diigo com tautao still waiting for broadband we are',\n",
       " 'wow tons of replies from you may have to unfollow so i can see my friends tweets you re scrolling the feed a lot',\n",
       " 'our duck and chicken are taking wayyy too long to hatch',\n",
       " 'put vacation photos online a few yrs ago pc crashed and now i forget the name of the site',\n",
       " 'i need a hug',\n",
       " 'not sure what they are only that they are pos as much as i want to i dont think can trade away company assets sorry andy',\n",
       " 'i hate when that happens',\n",
       " 'i have a sad feeling that dallas is not going to show up i gotta say though you d think more shows would use music from the game mmm',\n",
       " 'ugh degrees tomorrow',\n",
       " 'where did u move to i thought u were already in sd hmmm random u found me glad to hear yer doing well',\n",
       " 'i miss my ps it s out of commission wutcha playing have you copped blood on the sand',\n",
       " 'just leaving the parking lot of work',\n",
       " 'the life is cool but not for me',\n",
       " 'sadly though i ve never gotten to experience the post coitus cigarette before and now i never will',\n",
       " 'i had such a nice day too bad the rain comes in tomorrow at am',\n",
       " 'too bad i won t be around i lost my job and can t even pay my phone bill lmao aw shucks',\n",
       " 'damm back to school tomorrow',\n",
       " 'mo jobs no money how in the hell is min wage here f n clams an hour',\n",
       " 'not forever see you soon',\n",
       " 'algonquin agreed i saw the failwhale allllll day today',\n",
       " 'oh haha dude i dont really look at em unless someone says hey i added you sorry i m so terrible at that i need a pop up',\n",
       " 'i m sure you re right i need to start working out with you and the nikster or jared at least',\n",
       " 'i really hate how people diss my bands trace is clearly not ugly',\n",
       " 'gym attire today was puma singlet adidas shorts and black business socks and leather shoes lucky did not run into any cute girls',\n",
       " 'why won t you show my location',\n",
       " 'no picnic my phone smells like citrus',\n",
       " 'my donkey is sensitive about such comments nevertheless he d and me d be glad to see your mug asap charger is still awol',\n",
       " 'no new csi tonight fml',\n",
       " 'i think my arms are sore from tennis',\n",
       " 'wednesday my b day don t know what do',\n",
       " 'poor cameron the hills',\n",
       " 'pray for me please the ex is threatening to start sh at my our babies st birthday party what a jerk and i still have a headache',\n",
       " 'hmm do u really enjoy being with him if the problems are too constants u should think things more find someone ulike',\n",
       " 'strider is a sick little puppy',\n",
       " 'so rylee grace wana go steve s party or not sadly since its easter i wnt b able do much but ohh well',\n",
       " 'hey i actually won one of my bracket pools too bad it wasn t the one for money',\n",
       " 'you don t follow me either and i work for you',\n",
       " 'a bad nite for the favorite teams astros and spartans lose the nite out with t w was good',\n",
       " 'body of missing northern calif girl found police have found the remains of a missing northern california girl',\n",
       " 'i hope they will increase the capacity fast yesterday was such a pain got the fail whale times in hours',\n",
       " 'behind on my classes for work',\n",
       " 'watching house',\n",
       " 'remember my bum leg strikes back this time its serious',\n",
       " 'cool i will their are all kinds of complaints about this laptop online about overheating but no recalls',\n",
       " 'emily will be glad when mommy is done training at her new job she misses her',\n",
       " 'would rather the first party send bad messages than the rd party send mixed ones sophmore year all over again',\n",
       " 'it s overrated',\n",
       " 'q i know i heard it this afternoon and wondered the same thing moscow is so behind the times',\n",
       " 'laying in bed with no voice',\n",
       " 'i m sooo sad they killed off kutner on house whyyyyyyyy']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "testing = df.text[:100]\n",
    "test_result = []\n",
    "for t in testing:\n",
    "    test_result.append(tweet_cleaner(t))\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the tweets...\n",
      "\n",
      "Tweets 10000 of 400000 has been processed\n",
      "Tweets 20000 of 400000 has been processed\n",
      "Tweets 30000 of 400000 has been processed\n",
      "Tweets 40000 of 400000 has been processed\n",
      "Tweets 50000 of 400000 has been processed\n",
      "Tweets 60000 of 400000 has been processed\n",
      "Tweets 70000 of 400000 has been processed\n",
      "Tweets 80000 of 400000 has been processed\n",
      "Tweets 90000 of 400000 has been processed\n",
      "Tweets 100000 of 400000 has been processed\n",
      "Tweets 110000 of 400000 has been processed\n",
      "Tweets 120000 of 400000 has been processed\n",
      "Tweets 130000 of 400000 has been processed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-1523d289f633>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Tweets %d of %d has been processed\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnums\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mclean_tweet_texts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_cleaner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-47-db6d7b72035a>\u001b[0m in \u001b[0;36mtweet_cleaner\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mletters_only\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[^a-zA-Z]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mlower_case\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mletters_only\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlower_case\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/regexp.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# If our regexp matches tokens, use re.findall:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_regexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nums = [0,400000]\n",
    "print (\"Cleaning and parsing the tweets...\\n\")\n",
    "clean_tweet_texts = []\n",
    "for i in range(nums[0],nums[1]):\n",
    "    if( (i+1)%10000 == 0 ):\n",
    "        print (\"Tweets %d of %d has been processed\" % ( i+1, nums[1] ))\n",
    "    clean_tweet_texts.append(tweet_cleaner(df['text'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
